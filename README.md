# CMA-ES

Overview
Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is an evolutionary algorithm used for optimization problems. It is particularly effective for non-linear, non-convex, and high-dimensional optimization tasks. The algorithm adapts the covariance matrix of the search distribution, allowing it to efficiently explore the search space and converge to an optimal solution.


Search Distribution: CMA-ES models the search for the optimal solution using a multivariate normal distribution:
N(m_t, sigma_t^2 * C_t)
where:
m_t is the mean vector at iteration t.
sigma_t is the global step size at iteration t.
C_t is the covariance matrix at iteration t.

Population Generation: At each iteration, a population of candidate solutions is generated by sampling from the multivariate normal distribution:
x_k^(t+1) = m_t + sigma_t * B_t * D_t * z_k^(t)
where:
x_k^(t+1) is the k-th candidate solution in generation t+1.
z_k^(t) is a standard normally distributed random vector.
B_t is the matrix of eigenvectors of C_t.
D_t is the diagonal matrix of square roots of eigenvalues of C_t.

Selection and Recombination: The candidate solutions are evaluated using the objective function, and the best solutions are selected. The mean vector m_(t+1) for the next generation is updated as a weighted sum of the best candidate solutions:
m_(t+1) = sum(w_i * x_i^(t+1))
where:
w_i are the weights assigned to the top-ranking solutions.
x_i^(t+1) are the selected solutions.

Covariance Matrix Adaptation: The covariance matrix C_t is updated to reflect the distribution of the selected solutions. This update allows the algorithm to adapt the shape of the search distribution over time:
C_(t+1) = (1 - c1 - cmu) * C_t + c1 * (p_c * p_c^T) + cmu * sum(w_i * (x_i^(t+1) - m_t) * (x_i^(t+1) - m_t)^T)
where:
p_c is the evolution path.
c1 and cmu are learning rates.

Step Size Adaptation: The global step size sigma_t is adapted based on the evolution path p_s to control the exploration of the search space:
sigma_(t+1) = sigma_t * exp((cs / ds) * (||p_s|| / E(||N(0,I)||) - 1))
where:
p_s is the evolution path for step size control.
cs is the learning rate for p_s.
ds is the damping factor.
E(||N(0,I)||) is the expected length of a standard normal distribution vector.



Initialization:
Initialize the mean vector m_0, step size sigma_0, and covariance matrix C_0.
Initialize evolution paths p_c and p_s to zero vectors.

Iteration:
Sample Population: Generate a population of candidate solutions using the current mean, covariance matrix, and step size.
Evaluate Solutions: Evaluate the candidate solutions using the objective function.
Select and Recombine: Select the best solutions and update the mean vector.
Adapt Covariance Matrix: Update the covariance matrix to reflect the distribution of the selected solutions.
Adapt Step Size: Update the step size based on the evolution path.

Termination:
The algorithm terminates when a stopping criterion is met, such as a maximum number of iterations or convergence of the mean vector.

Factors
Population Size: The number of candidate solutions generated at each iteration.
Step Size (sigma_t): Controls the exploration rate; larger values allow broader search, while smaller values focus on local search.
Covariance Matrix (C_t): Models the dependencies between variables and is adapted to improve the search direction.

Pros
Adaptation to the Problem: CMA-ES automatically adapts the search distribution based on the problem landscape.
Robustness: Effective for a wide range of optimization problems, including those that are non-linear and non-convex.
Scalability: Works well in high-dimensional spaces.

Cons
Computational Cost: The algorithm can be computationally expensive, especially for large populations or high-dimensional problems.
Complexity: Implementation and parameter tuning can be complex compared to simpler algorithms like gradient descent.
